#!/usr/bin/env python
"""
vig.py - Video Info Getter CLI
ç”¨é€”ï¼šçˆ¬å– Bilibili å’Œ YouTube è§†é¢‘çš„ä¿¡æ¯ï¼ˆä¸Šä¼ ä½œè€…ã€ä¸Šä¼ æ—¥æœŸï¼‰
ä½¿ç”¨ Crawl4AI è¿›è¡Œç½‘é¡µçˆ¬å–
"""
import argparse
import asyncio
import re
import sys
import json
import logging
import os
from dataclasses import dataclass
from typing import Optional
from urllib.parse import urlparse, parse_qs
from http.cookiejar import MozillaCookieJar

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class VideoInfo:
    """è§†é¢‘ä¿¡æ¯æ•°æ®ç±»"""

    platform: str
    video_id: str
    author: str
    upload_date: str
    title: Optional[str] = None
    url: Optional[str] = None


class VideoInfoGetter:
    """è§†é¢‘ä¿¡æ¯è·å–å™¨"""

    # URL æ¨¡å¼åŒ¹é…
    BILIBILI_PATTERNS = [
        r"bilibili\.com/video/(BV[\w]+)",
        r"bilibili\.com/video/(av\d+)",
        r"b23\.tv/([\w]+)",
    ]

    YOUTUBE_PATTERNS = [
        r"youtube\.com/watch\?v=([\w-]+)",
        r"youtu\.be/([\w-]+)",
        r"youtube\.com/shorts/([\w-]+)",
    ]

    def __init__(self):
        self.crawler = None
        self.cookies = self._load_cookies()

    def _load_cookies(self) -> list:
        """
        ä» cookies.txt æ–‡ä»¶åŠ è½½ cookiesï¼ˆNetscape/Mozilla æ ¼å¼ï¼‰
        è¿”å› Crawl4AI æ‰€éœ€çš„ cookies åˆ—è¡¨æ ¼å¼
        """
        cookies_file = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "cookies.txt"
        )
        cookies = []

        if not os.path.exists(cookies_file):
            logger.debug("cookies.txt æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä¸ä½¿ç”¨ cookies")
            return cookies

        try:
            cookie_jar = MozillaCookieJar(cookies_file)
            cookie_jar.load(ignore_discard=True, ignore_expires=True)

            for cookie in cookie_jar:
                cookies.append(
                    {
                        "name": cookie.name,
                        "value": cookie.value,
                        "domain": cookie.domain,
                        "path": cookie.path,
                        "secure": cookie.secure,
                    }
                )

            logger.info(f"æˆåŠŸä» cookies.txt åŠ è½½äº† {len(cookies)} ä¸ª cookies")
        except Exception as e:
            logger.warning(f"åŠ è½½ cookies.txt å¤±è´¥: {e}")

        return cookies

    def detect_platform(self, url: str) -> tuple[str, str]:
        """
        æ£€æµ‹ URL å¯¹åº”çš„å¹³å°å’Œè§†é¢‘ ID
        è¿”å›: (platform, video_id)
        """
        # æ£€æŸ¥ Bilibili
        for pattern in self.BILIBILI_PATTERNS:
            match = re.search(pattern, url)
            if match:
                return "bilibili", match.group(1)

        # æ£€æŸ¥ YouTube
        for pattern in self.YOUTUBE_PATTERNS:
            match = re.search(pattern, url)
            if match:
                return "youtube", match.group(1)

        # å°è¯•ä»æŸ¥è¯¢å‚æ•°è·å– YouTube video ID
        parsed = urlparse(url)
        if "youtube.com" in parsed.netloc:
            query_params = parse_qs(parsed.query)
            if "v" in query_params:
                return "youtube", query_params["v"][0]

        return "unknown", ""

    async def _init_crawler(self):
        """åˆå§‹åŒ– Crawl4AI çˆ¬è™«"""
        if self.crawler is None:
            try:
                from crawl4ai import AsyncWebCrawler, BrowserConfig

                # é…ç½®æµè§ˆå™¨ï¼ŒåŒ…å« cookies
                browser_config = BrowserConfig(
                    headless=True,
                    cookies=self.cookies if self.cookies else None,
                )
                self.crawler = AsyncWebCrawler(config=browser_config)
                await self.crawler.start()
            except ImportError:
                print("Error: crawl4ai not installed.")
                print("Please install it using: pip install crawl4ai")
                sys.exit(1)

    async def _close_crawler(self):
        """å…³é—­çˆ¬è™«"""
        if self.crawler:
            await self.crawler.close()
            self.crawler = None

    async def get_bilibili_info(self, video_id: str) -> Optional[VideoInfo]:
        """
        è·å– Bilibili è§†é¢‘ä¿¡æ¯
        """
        await self._init_crawler()

        url = f"https://www.bilibili.com/video/{video_id}"
        print(f"[Bilibili] æ­£åœ¨çˆ¬å–: {url}")

        try:
            from crawl4ai import CrawlerRunConfig

            config = CrawlerRunConfig(
                wait_until="domcontentloaded",
                page_timeout=30000,
            )

            result = await self.crawler.arun(url=url, config=config)

            if not result.success:
                logger.error(f"çˆ¬å–å¤±è´¥: {result.error_message}")
                return None

            html = result.html

            # æå–ä½œè€…ä¿¡æ¯ - ä» meta æ ‡ç­¾æˆ–é¡µé¢å†…å®¹ä¸­æå–
            author = self._extract_bilibili_author(html)

            # æå–ä¸Šä¼ æ—¥æœŸ
            upload_date = self._extract_bilibili_date(html)

            # æå–æ ‡é¢˜
            title = self._extract_bilibili_title(html)

            return VideoInfo(
                platform="Bilibili",
                video_id=video_id,
                author=author or "æœªçŸ¥",
                upload_date=upload_date or "æœªçŸ¥",
                title=title,
                url=url,
            )

        except Exception as e:
            logger.error(f"è·å– Bilibili è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def _extract_bilibili_author(self, html: str) -> Optional[str]:
        """ä» Bilibili HTML ä¸­æå–ä½œè€…"""
        patterns = [
            # meta æ ‡ç­¾ä¸­çš„ä½œè€…
            r'<meta\s+name="author"\s+content="([^"]+)"',
            # JSON-LD æ•°æ®ä¸­çš„ä½œè€…
            r'"uploader":\s*{\s*"name":\s*"([^"]+)"',
            r'"owner":\s*{\s*[^}]*"name":\s*"([^"]+)"',
            # é¡µé¢å…ƒç´ ä¸­çš„ä½œè€…å
            r'class="up-name[^"]*"[^>]*>([^<]+)<',
            r'class="username"[^>]*>([^<]+)<',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1).strip()

        # å°è¯•ä» __INITIAL_STATE__ ä¸­æå–
        initial_state_match = re.search(
            r"__INITIAL_STATE__\s*=\s*({.*?});", html, re.DOTALL
        )
        if initial_state_match:
            try:
                data = json.loads(initial_state_match.group(1))
                if "videoData" in data and "owner" in data["videoData"]:
                    return data["videoData"]["owner"].get("name")
            except json.JSONDecodeError:
                pass

        return None

    def _extract_bilibili_date(self, html: str) -> Optional[str]:
        """ä» Bilibili HTML ä¸­æå–ä¸Šä¼ æ—¥æœŸ"""
        patterns = [
            # å¸¸è§çš„æ—¥æœŸæ ¼å¼
            r'"pubdate":\s*(\d+)',
            r'"ctime":\s*(\d+)',
            r'class="pubdate-text"[^>]*>([^<]+)<',
            r'class="pudate-text"[^>]*>([^<]+)<',
            # é¡µé¢ä¸­çš„æ—¥æœŸæ˜¾ç¤º
            r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2})",
            r"(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)",
        ]

        for i, pattern in enumerate(patterns):
            match = re.search(pattern, html)
            if match:
                value = match.group(1)
                # å¦‚æœæ˜¯æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºæ—¥æœŸ
                if i < 2 and value.isdigit():
                    from datetime import datetime

                    try:
                        dt = datetime.fromtimestamp(int(value))
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
                    except:
                        pass
                return value.strip()

        # å°è¯•ä» __INITIAL_STATE__ ä¸­æå–
        initial_state_match = re.search(
            r"__INITIAL_STATE__\s*=\s*({.*?});", html, re.DOTALL
        )
        if initial_state_match:
            try:
                data = json.loads(initial_state_match.group(1))
                if "videoData" in data:
                    pubdate = data["videoData"].get("pubdate")
                    if pubdate:
                        from datetime import datetime

                        dt = datetime.fromtimestamp(int(pubdate))
                        return dt.strftime("%Y-%m-%d %H:%M:%S")
            except (json.JSONDecodeError, ValueError):
                pass

        return None

    def _extract_bilibili_title(self, html: str) -> Optional[str]:
        """ä» Bilibili HTML ä¸­æå–æ ‡é¢˜"""
        patterns = [
            r"<title>([^<]+)</title>",
            r'<meta\s+property="og:title"\s+content="([^"]+)"',
            r'"title":\s*"([^"]+)"',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                title = match.group(1).strip()
                # æ¸…ç† Bilibili æ ‡é¢˜åç¼€
                title = re.sub(r"_å“”å“©å“”å“©_bilibili$", "", title)
                title = re.sub(r"-å“”å“©å“”å“©$", "", title)
                return title

        return None

    async def get_youtube_info(self, video_id: str) -> Optional[VideoInfo]:
        """
        è·å– YouTube è§†é¢‘ä¿¡æ¯
        """
        await self._init_crawler()

        url = f"https://www.youtube.com/watch?v={video_id}"
        print(f"[YouTube] æ­£åœ¨çˆ¬å–: {url}")

        try:
            from crawl4ai import CrawlerRunConfig

            config = CrawlerRunConfig(
                wait_until="domcontentloaded",
                page_timeout=30000,
            )

            result = await self.crawler.arun(url=url, config=config)

            if not result.success:
                logger.error(f"çˆ¬å–å¤±è´¥: {result.error_message}")
                return None

            html = result.html

            # æå–ä½œè€…ä¿¡æ¯
            author = self._extract_youtube_author(html)

            # æå–ä¸Šä¼ æ—¥æœŸ
            upload_date = self._extract_youtube_date(html)

            # æå–æ ‡é¢˜
            title = self._extract_youtube_title(html)

            return VideoInfo(
                platform="YouTube",
                video_id=video_id,
                author=author or "æœªçŸ¥",
                upload_date=upload_date or "æœªçŸ¥",
                title=title,
                url=url,
            )

        except Exception as e:
            logger.error(f"è·å– YouTube è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
            return None

    def _extract_youtube_author(self, html: str) -> Optional[str]:
        """ä» YouTube HTML ä¸­æå–ä½œè€…"""
        patterns = [
            # JSON-LD æ•°æ®
            r'"author":\s*"([^"]+)"',
            r'"ownerChannelName":\s*"([^"]+)"',
            r'"channelName":\s*"([^"]+)"',
            # meta æ ‡ç­¾
            r'<link\s+itemprop="name"\s+content="([^"]+)"',
            r'"name":\s*"([^"]+)"[^}]*"@type":\s*"Person"',
            # ytInitialData ä¸­çš„æ•°æ®
            r'"videoOwnerRenderer"[^}]*"title"[^}]*"runs"[^}]*"text":\s*"([^"]+)"',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                return match.group(1).strip()

        # å°è¯•ä» ytInitialPlayerResponse ä¸­æå–
        player_response_match = re.search(
            r"ytInitialPlayerResponse\s*=\s*({.*?});", html, re.DOTALL
        )
        if player_response_match:
            try:
                # ç®€å•æå– author å­—æ®µ
                author_match = re.search(
                    r'"author":\s*"([^"]+)"', player_response_match.group(1)
                )
                if author_match:
                    return author_match.group(1)
            except:
                pass

        return None

    def _extract_youtube_date(self, html: str) -> Optional[str]:
        """ä» YouTube HTML ä¸­æå–ä¸Šä¼ æ—¥æœŸ"""
        patterns = [
            # JSON æ•°æ®ä¸­çš„æ—¥æœŸ
            r'"uploadDate":\s*"([^"]+)"',
            r'"publishDate":\s*"([^"]+)"',
            r'"dateText"[^}]*"simpleText":\s*"([^"]+)"',
            # meta æ ‡ç­¾
            r'<meta\s+itemprop="uploadDate"\s+content="([^"]+)"',
            r'<meta\s+itemprop="datePublished"\s+content="([^"]+)"',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                date_str = match.group(1).strip()
                # å°è¯•æ ¼å¼åŒ–æ—¥æœŸ
                try:
                    from datetime import datetime

                    # å°è¯•è§£æ ISO æ ¼å¼
                    if "T" in date_str:
                        dt = datetime.fromisoformat(date_str.replace("Z", "+00:00"))
                        return dt.strftime("%Y-%m-%d")
                except:
                    pass
                return date_str

        return None

    def _extract_youtube_title(self, html: str) -> Optional[str]:
        """ä» YouTube HTML ä¸­æå–æ ‡é¢˜"""
        patterns = [
            r'<meta\s+property="og:title"\s+content="([^"]+)"',
            r"<title>([^<]+)</title>",
            r'"title":\s*"([^"]+)"',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                title = match.group(1).strip()
                # æ¸…ç† YouTube æ ‡é¢˜åç¼€
                title = re.sub(r"\s*-\s*YouTube$", "", title)
                return title

        return None

    async def get_video_info(self, url: str) -> Optional[VideoInfo]:
        """
        æ ¹æ® URL è‡ªåŠ¨è¯†åˆ«å¹³å°å¹¶è·å–è§†é¢‘ä¿¡æ¯
        """
        platform, video_id = self.detect_platform(url)

        if platform == "bilibili":
            return await self.get_bilibili_info(video_id)
        elif platform == "youtube":
            return await self.get_youtube_info(video_id)
        else:
            logger.error(f"æ— æ³•è¯†åˆ«çš„è§†é¢‘å¹³å°: {url}")
            return None

    async def process_urls(self, urls: list[str], output_format: str = "text"):
        """
        æ‰¹é‡å¤„ç†è§†é¢‘ URL
        """
        results = []

        try:
            for url in urls:
                info = await self.get_video_info(url)
                if info:
                    results.append(info)
                print("-" * 50)
        finally:
            await self._close_crawler()

        # è¾“å‡ºç»“æœ
        self._output_results(results, output_format)

        return results

    def _output_results(self, results: list[VideoInfo], output_format: str):
        """è¾“å‡ºç»“æœ"""
        if not results:
            print("\næ²¡æœ‰è·å–åˆ°ä»»ä½•è§†é¢‘ä¿¡æ¯ã€‚")
            return

        print("\n" + "=" * 60)
        print("ğŸ“¹ è§†é¢‘ä¿¡æ¯æ±‡æ€»")
        print("=" * 60)

        if output_format == "json":
            import json

            data = [
                {
                    "platform": r.platform,
                    "video_id": r.video_id,
                    "author": r.author,
                    "upload_date": r.upload_date,
                    "title": r.title,
                    "url": r.url,
                }
                for r in results
            ]
            print(json.dumps(data, ensure_ascii=False, indent=2))
        else:
            for i, info in enumerate(results, 1):
                print(f"\n[{i}] {info.platform}")
                if info.title:
                    print(f"    æ ‡é¢˜: {info.title}")
                print(f"    ä½œè€…: {info.author}")
                print(f"    ä¸Šä¼ æ—¥æœŸ: {info.upload_date}")
                print(f"    è§†é¢‘ID: {info.video_id}")
                if info.url:
                    print(f"    é“¾æ¥: {info.url}")


def main():
    parser = argparse.ArgumentParser(
        description="Video Info Getter - è·å– Bilibili/YouTube è§†é¢‘ä¿¡æ¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python vig.py https://www.bilibili.com/video/BV1xx411c7mD
  python vig.py https://www.youtube.com/watch?v=dQw4w9WgXcQ
  python vig.py url1 url2 url3 --format json
  python vig.py urls.txt
        """,
    )

    parser.add_argument(
        "urls", nargs="+", help="è§†é¢‘ URL æˆ–åŒ…å« URL åˆ—è¡¨çš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "-f",
        "--format",
        default="text",
        choices=["text", "json"],
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: text)",
    )
    parser.add_argument("-v", "--verbose", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—")

    args = parser.parse_args()

    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # å¤„ç†è¾“å…¥çš„ URL
    urls = []
    import os

    for url_arg in args.urls:
        if os.path.isfile(url_arg):
            with open(url_arg, "r", encoding="utf-8") as f:
                urls.extend(
                    [
                        line.strip()
                        for line in f
                        if line.strip() and not line.startswith("#")
                    ]
                )
        else:
            urls.append(url_arg)

    if not urls:
        print("é”™è¯¯: æœªæä¾›ä»»ä½•æœ‰æ•ˆçš„è§†é¢‘ URL")
        sys.exit(1)

    print(f"ğŸ“‹ å…± {len(urls)} ä¸ªè§†é¢‘å¾…å¤„ç†\n")

    # åˆ›å»ºè·å–å™¨å¹¶å¤„ç†
    getter = VideoInfoGetter()
    asyncio.run(getter.process_urls(urls, args.format))


if __name__ == "__main__":
    main()
